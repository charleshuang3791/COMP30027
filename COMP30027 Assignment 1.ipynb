{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Assignment 1: Pose classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `987165`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "DUD_LABEL = 9999\n",
    "POSES_NUM = 10\n",
    "\n",
    "def convert(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        try:\n",
    "            return float(value)\n",
    "        except:\n",
    "            return value\n",
    "\n",
    "def label_dud(value):\n",
    "    if value in ['','?']:\n",
    "        return DUD_LABEL\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def clean(value):\n",
    "    return convert(value)\n",
    "#     return convert(label_dud(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess(file_name):\n",
    "    atts = []\n",
    "    classes = []\n",
    "    \n",
    "    # taken from workshop\n",
    "    with open(file_name, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            all_columns = line.strip().split(\",\")\n",
    "            \n",
    "            # the attributes are the 11 coordinates that come after the class, in each row\n",
    "            atts_sing = all_columns[1:]\n",
    "            \n",
    "            # default string values are cleaned\n",
    "            for i in range(len(atts_sing)):\n",
    "                atts_sing[i] = clean(atts_sing[i])\n",
    "            class_val = clean(all_columns[0])\n",
    "            \n",
    "            # append each line into their respective lists\n",
    "            classes.append(class_val)\n",
    "            atts.append(atts_sing)\n",
    "    \n",
    "    return {'atts': atts, 'classes': classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(train_atts, train_classes):\n",
    "    class_freq = defaultdict(int)\n",
    "    class_priors = {}\n",
    "    mean_sd_array = {}\n",
    "    entries_by_label = {}\n",
    "    instances = 0\n",
    "    \n",
    "    train_rows = len(train_atts)\n",
    "    \n",
    "    # taken from workshop\n",
    "    for class_inst in train_classes:\n",
    "        instances += 1\n",
    "        class_freq[class_inst] += 1\n",
    "    # taken from workshop\n",
    "    \n",
    "    for lbl in class_freq.keys():\n",
    "        # making class_priors\n",
    "        class_priors[lbl] = class_freq[lbl] / instances\n",
    "        \n",
    "        # making mean_sd_array\n",
    "        mean_sd_array[lbl] = {}\n",
    "        \n",
    "    for lbl in class_freq.keys():\n",
    "        entries_by_label[lbl] = []\n",
    "        for row in range(train_rows):\n",
    "            if train_classes[row] == lbl:\n",
    "                entries_by_label[lbl].append(train_atts[row])\n",
    "    \n",
    "    mean_sd_array = find_mean_sd('mean', class_priors, entries_by_label, mean_sd_array)\n",
    "    mean_sd_array = find_mean_sd('sd', class_priors, entries_by_label, mean_sd_array)        \n",
    "    return {'class_priors': class_priors, 'mean_sd_array': mean_sd_array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_sd(mean_or_sd, class_priors, entries_by_label, mean_sd_array):\n",
    "    for lbl in class_priors.keys():\n",
    "        rows_sub = len(entries_by_label[lbl])\n",
    "        cols_sub = len(entries_by_label[lbl][0])\n",
    "        for col in range(cols_sub):\n",
    "            if mean_or_sd == 'mean':\n",
    "                mean_sd_array[lbl][col] = [0.0, 0.0]\n",
    "                mean = 0.0\n",
    "            elif mean_or_sd == 'sd':\n",
    "                sd = 0.0\n",
    "            count = 0\n",
    "            for row in range(rows_sub):\n",
    "                plucked_value = entries_by_label[lbl][row][col]\n",
    "                # if the picked value is not a dud\n",
    "                if plucked_value != DUD_LABEL:\n",
    "                    if mean_or_sd == 'mean':\n",
    "                        mean += plucked_value\n",
    "                    elif mean_or_sd == 'sd':\n",
    "                        mean = mean_sd_array[lbl][col][0]\n",
    "                        sd += math.pow(entries_by_label[lbl][row][col] - mean, 2)    \n",
    "                    count += 1\n",
    "            # insufficient data to calculate a mean or sd\n",
    "            if count == 0 or count == 1:\n",
    "                mean_sd_array[lbl][col][0] = 0\n",
    "                mean_sd_array[lbl][col][1] = 0\n",
    "            else:\n",
    "                if mean_or_sd == 'mean':\n",
    "                    mean_sd_array[lbl][col][0] = mean / count\n",
    "                elif mean_or_sd == 'sd':\n",
    "                    mean_sd_array[lbl][col][1] = math.sqrt(sd / (count - 1))\n",
    "    return mean_sd_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset\n",
    "\n",
    "def predict(test_atts, class_priors, mean_sd_array):\n",
    "    prediction = [] # prediction output list\n",
    "    class_probabilities = []\n",
    "    \n",
    "    rows = len(test_atts)\n",
    "    cols = len(test_atts[0])\n",
    "    \n",
    "    for row in range(rows):\n",
    "        class_probabilities.append({})\n",
    "        for lbl in class_priors.keys():\n",
    "            sum = math.log(class_priors[lbl])\n",
    "            for col in range(cols):\n",
    "                if test_atts[row][col] != DUD_LABEL:\n",
    "                    # some likelihoods cannot be calculated because the training data had\n",
    "                    # too many missing features to calculate a gaussian mean and sd.\n",
    "                    if mean_sd_array[lbl][col][1] != 0:\n",
    "                        # some probabilities are too small for the log, thereby necessitating\n",
    "                        # this try and except\n",
    "                        try:\n",
    "                            sum = sum + math.log(calculate_pdf(test_atts[row][col], mean_sd_array[lbl][col][0], mean_sd_array[lbl][col][1]))\n",
    "                        except:\n",
    "                            pass\n",
    "            class_probabilities[row][lbl] = sum\n",
    "    \n",
    "    for class_set in class_probabilities:\n",
    "        max_class = ''\n",
    "        max_prob = -9999999\n",
    "\n",
    "        # compares probabilities to find the highest one\n",
    "        for lbl in class_set.keys():\n",
    "            current_prob = class_set[lbl]\n",
    "            if current_prob > max_prob:\n",
    "                max_prob = current_prob\n",
    "                max_class = lbl\n",
    "        prediction.append(max_class)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates a pdf given the normal assumption\n",
    "def calculate_pdf(x_value, mean, sd):\n",
    "    try:\n",
    "        return (1 / (sd * math.sqrt(2 * math.pi))) * math.pow(math.e, -1/2 * math.pow((x_value - mean) / sd, 2))\n",
    "    except ZeroDivisionError:\n",
    "        calculate_pdf(x_value, mean, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "# essentially, this function finds the accuracy\n",
    "def evaluate(prediction, test_classes):\n",
    "    correct_count = 0\n",
    "    rows = len(prediction)\n",
    "    \n",
    "    for row in range(rows):\n",
    "        if prediction[row] == test_classes[row]:\n",
    "            correct_count = correct_count + 1\n",
    "            \n",
    "    return correct_count / rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.7155172413793104\n"
     ]
    }
   ],
   "source": [
    "training_data = preprocess(\"train.csv\")\n",
    "testing_data = preprocess(\"test.csv\")\n",
    "\n",
    "probability_data = train(training_data['atts'], training_data['classes'])\n",
    "\n",
    "myprediction = predict(testing_data['atts'], probability_data['class_priors'], probability_data['mean_sd_array'])\n",
    "\n",
    "evaluation_data = evaluate(myprediction, testing_data['classes'])\n",
    "\n",
    "print(\"accuracy is\", evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to **two** questions of your choosing.\n",
    "\n",
    "If you are in a group of 2, you will respond to **four** questions of your choosing.\n",
    "\n",
    "A response to a question should take about 100â€“250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer should be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Model Evaluation\n",
    "Since this is a multiclass classification problem, there are multiple ways to compute precision, recall, and F-score for this classifier. Implement at least two of the methods from the \"Model Evaluation\" lecture and discuss any differences between them. (The implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this constructs the actual vs predicted array\n",
    "def return_actual_vs_predicted(actual_classes, predicted_classes):\n",
    "    # creating our 2D array, ready to be filled\n",
    "    actual_vs_predicted = []\n",
    "    initialised_row = []\n",
    "    for i in range(POSES_NUM):\n",
    "        initialised_row.append(0)\n",
    "    \n",
    "    poses_names = [\"bridge\", \"childs\", \"downwarddog\", \"mountain\", \"plank\", \"seatedforwardbend\", \"tree\", \"trianglepose\", \"warrior1\", \"warrior2\"]\n",
    "\n",
    "    for i in range(POSES_NUM):\n",
    "        actual_vs_predicted.append(initialised_row.copy())\n",
    "\n",
    "    for myindex, actual_class in enumerate(actual_classes):\n",
    "        try:\n",
    "            row_number = poses_names.index(actual_class)\n",
    "        except:\n",
    "            print(\"unexpected actual class\")\n",
    "        \n",
    "        predicted_class = predicted_classes[myindex]\n",
    "        \n",
    "        try:\n",
    "            col_number = poses_names.index(predicted_class)\n",
    "        except:\n",
    "            print(\"unexpected predicted class\")\n",
    "        \n",
    "        # this iterates through and counts the instances\n",
    "        actual_vs_predicted[row_number][col_number] += 1\n",
    "    return actual_vs_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 5, 0, 1, 0, 0, 0, 0, 0], [0, 11, 0, 1, 0, 1, 0, 0, 0, 0], [1, 1, 13, 3, 0, 0, 0, 0, 0, 0], [0, 0, 0, 26, 0, 0, 4, 0, 0, 0], [2, 0, 0, 0, 6, 0, 0, 1, 0, 0], [0, 3, 1, 0, 0, 4, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 3, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 4, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 7]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3  4  5  6  7  8  9\n",
      "0  5   3   5   0  1  0  0  0  0  0\n",
      "1  0  11   0   1  0  1  0  0  0  0\n",
      "2  1   1  13   3  0  0  0  0  0  0\n",
      "3  0   0   0  26  0  0  4  0  0  0\n",
      "4  2   0   0   0  6  0  0  1  0  0\n",
      "5  0   3   1   0  0  4  0  1  0  0\n",
      "6  1   0   0   0  0  0  3  0  2  0\n",
      "7  0   0   0   0  0  0  0  4  0  0\n",
      "8  0   0   0   0  1  0  0  0  4  0\n",
      "9  0   0   0   0  0  0  1  0  0  7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "myactual_vs_predicted = return_actual_vs_predicted(testing_data['classes'], myprediction)\n",
    "print(myactual_vs_predicted)\n",
    "\n",
    "# better visualisations below\n",
    "plt.imshow(myactual_vs_predicted)\n",
    "plt.show()\n",
    "\n",
    "print(pd.DataFrame(myactual_vs_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_macro(actual_vs_predicted):\n",
    "    precisions_list = []\n",
    "    for i in range(POSES_NUM):\n",
    "        precisions_list.append(0)\n",
    "        \n",
    "    for pose_number in range(POSES_NUM):\n",
    "        tp = actual_vs_predicted[pose_number][pose_number]\n",
    "        fp = 0\n",
    "        for myrow_num, myrow in enumerate(actual_vs_predicted):\n",
    "            if myrow_num != pose_number:\n",
    "                fp += myrow[pose_number]\n",
    "        precisions_list[pose_number] = tp / (tp + fp)\n",
    "    \n",
    "    return sum(precisions_list) / POSES_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6975877192982457\n"
     ]
    }
   ],
   "source": [
    "myprecision_macro = precision_macro(myactual_vs_predicted)\n",
    "print(myprecision_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_micro(actual_vs_predicted):\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "        \n",
    "    for pose_number in range(POSES_NUM):\n",
    "        tp = actual_vs_predicted[pose_number][pose_number]\n",
    "        fp = 0\n",
    "        for myrow_num, myrow in enumerate(actual_vs_predicted):\n",
    "            if myrow_num != pose_number:\n",
    "                fp += myrow[pose_number]\n",
    "        tp_list.append(tp)\n",
    "        fp_list.append(fp)\n",
    "    denom = [a_i + b_i for a_i, b_i in zip(tp_list, fp_list)]\n",
    "    return sum(tp_list) / sum(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7155172413793104\n"
     ]
    }
   ],
   "source": [
    "myprecision_micro = precision_micro(myactual_vs_predicted)\n",
    "print(myprecision_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_macro(actual_vs_predicted):\n",
    "    recalls_list = []\n",
    "    for i in range(POSES_NUM):\n",
    "        recalls_list.append(0)\n",
    "        \n",
    "    for pose_number in range(POSES_NUM):\n",
    "        tp = actual_vs_predicted[pose_number][pose_number]\n",
    "        fn = 0\n",
    "        \n",
    "        myrow = actual_vs_predicted[pose_number]\n",
    "        \n",
    "        for myindex, count in enumerate(myrow):\n",
    "            if myindex != pose_number:\n",
    "                fn += count\n",
    "\n",
    "        recalls_list[pose_number] = tp / (tp + fn)\n",
    "    \n",
    "    return sum(recalls_list) / POSES_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7078296703296703\n"
     ]
    }
   ],
   "source": [
    "myrecall_macro = recall_macro(myactual_vs_predicted)\n",
    "print(myrecall_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_micro(actual_vs_predicted):\n",
    "    tp_list = []\n",
    "    fn_list = []\n",
    "        \n",
    "    for pose_number in range(POSES_NUM):\n",
    "        tp = actual_vs_predicted[pose_number][pose_number]\n",
    "        fn = 0\n",
    "        \n",
    "        myrow = actual_vs_predicted[pose_number]\n",
    "        \n",
    "        for myindex, count in enumerate(myrow):\n",
    "            if myindex != pose_number:\n",
    "                fn += count\n",
    "        tp_list.append(tp)\n",
    "        fn_list.append(fn)\n",
    "    denom = [a_i + b_i for a_i, b_i in zip(tp_list, fn_list)]\n",
    "    return sum(tp_list) / sum(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7155172413793104\n"
     ]
    }
   ],
   "source": [
    "myrecall_micro = recall_micro(myactual_vs_predicted)\n",
    "print(myrecall_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score_calc(beta, precision, recall):\n",
    "    beta_squared = math.pow(beta, 2)\n",
    "    return ((1 + beta_squared) * precision * recall) / (beta_squared * precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7026713758076242\n",
      "0.7155172413793104\n"
     ]
    }
   ],
   "source": [
    "fscore_macro = f_score_calc(1, myprecision_macro, myrecall_macro)\n",
    "fscore_micro = f_score_calc(1, myprecision_micro, myrecall_micro)\n",
    "\n",
    "print(fscore_macro)\n",
    "print(fscore_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Engineer your own pose features from the provided keypoints. Instead of using the (x,y) positions of keypoints, you might consider the angles of the limbs or body, or the distances between pairs of keypoints. How does a naive Bayes classifier based on your engineered features compare to the classifier using (x,y) values? Please note that we are interested in explainable features for pose recognition, so simply putting the (x,y) values in a neural network or similar to get an arbitrary embedding will not receive full credit for this question. You should be able to explain the rationale behind your proposed features. Also, don't forget the conditional independence assumption of naive Bayes when proposing new features -- a large set of highly-correlated features may not work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Using both Gradients and Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_gradients_distances_2D(my_data):\n",
    "    whole_2D_array = []\n",
    "    for my_row in my_data['atts']:\n",
    "        my_gradients_distances = []\n",
    "        for body_part_number in range(10):\n",
    "            x1 = my_row[2 * body_part_number]\n",
    "            y1 = my_row[2 * body_part_number + 1]\n",
    "            x2 = my_row[2 * body_part_number + 2]\n",
    "            y2 = my_row[2 * body_part_number + 3]\n",
    "            if x1 != DUD_LABEL and y1 != DUD_LABEL and x2 != DUD_LABEL and y2 != DUD_LABEL:\n",
    "                x_diff = x2 - x1\n",
    "                if x_diff != 0:\n",
    "                    my_gradient = (y2 - y1) / (x2 - x1)\n",
    "                else:\n",
    "                    # if two points are on top of each other, set their gradients to be very high\n",
    "                    my_gradient = 9999999\n",
    "                a = [x1, y1]\n",
    "                b = [x2, y2]\n",
    "                jesuschrist = [a_i - b_i for a_i, b_i in zip(a, b)]\n",
    "                my_distance = np.linalg.norm(jesuschrist)\n",
    "            else:\n",
    "                my_gradient = DUD_LABEL\n",
    "                my_distance = DUD_LABEL\n",
    "            my_gradients_distances.append(my_gradient)\n",
    "            my_gradients_distances.append(my_distance)\n",
    "        whole_2D_array.append(my_gradients_distances)\n",
    "    return whole_2D_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_data = give_gradients_distances_2D(testing_data)\n",
    "processed_training_data = give_gradients_distances_2D(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Using Only Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_gradients_2D(my_data):\n",
    "    whole_2D_array = []\n",
    "    for my_row in my_data['atts']:\n",
    "        my_gradients = []\n",
    "        for body_part_number in range(10):\n",
    "            x1 = my_row[2 * body_part_number]\n",
    "            y1 = my_row[2 * body_part_number + 1]\n",
    "            x2 = my_row[2 * body_part_number + 2]\n",
    "            y2 = my_row[2 * body_part_number + 3]\n",
    "            if x1 != DUD_LABEL and y1 != DUD_LABEL and x2 != DUD_LABEL and y2 != DUD_LABEL:\n",
    "                x_diff = x2 - x1\n",
    "                if x_diff != 0:\n",
    "                    my_gradient = (y2 - y1) / (x2 - x1)\n",
    "                else:\n",
    "                    # if two points are on top of each other, set their gradients to be very high\n",
    "                    my_gradient = 9999999\n",
    "            else:\n",
    "                my_gradient = DUD_LABEL\n",
    "            my_gradients.append(my_gradient)\n",
    "        whole_2D_array.append(my_gradients)\n",
    "    return whole_2D_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_data = give_gradients_2D(testing_data)\n",
    "processed_training_data = give_gradients_2D(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Final Execution for Part 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.4396551724137931\n"
     ]
    }
   ],
   "source": [
    "# test instances with not a single feature able to be engineered are assigned the basic\n",
    "# classifier's prediction (placeholder: KLAI8273)\n",
    "basic_classifier_override = []\n",
    "\n",
    "for myindex, myrow in enumerate(processed_test_data):\n",
    "    replace_row = 1\n",
    "    for my_value in myrow:\n",
    "        if my_value != DUD_LABEL:\n",
    "            replace_row = 0\n",
    "    \n",
    "    if replace_row == 1:\n",
    "        basic_classifier_override.append(myprediction[myindex])\n",
    "    else:\n",
    "        basic_classifier_override.append(DUD_LABEL)\n",
    "\n",
    "probability_data2 = train(processed_training_data, training_data['classes'])\n",
    "myprediction2 = predict(processed_test_data, probability_data2['class_priors'], probability_data2['mean_sd_array'])\n",
    "\n",
    "# myprediction2 has some predictions overriden (see placeholder KLAI8273 for details)\n",
    "for my_index, prediction in enumerate(basic_classifier_override):\n",
    "    if prediction != DUD_LABEL:\n",
    "        myprediction2[my_index] = prediction\n",
    "\n",
    "evaluation_data = evaluate(myprediction2, testing_data['classes'])\n",
    "\n",
    "\"\"\"\n",
    "print(myprediction2)\n",
    "print(testing_data['classes'])\n",
    "\n",
    "for prediction_index in range(len(myprediction2)):\n",
    "    if testing_data['classes'][prediction_index] == myprediction2[prediction_index]:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"accuracy is\", evaluation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
